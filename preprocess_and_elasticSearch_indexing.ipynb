{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b140bd17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Downloading dataset...\n",
      "üìÇ Path to dataset files: C:\\Users\\Atharv\\.cache\\kagglehub\\datasets\\Cornell-University\\arxiv\\versions\\227\n",
      "üßº Preprocessing data...\n",
      "üîå Connecting to Elasticsearch...\n",
      "‚úÖ Successfully connected to Elasticsearch via HTTP!\n",
      "üìÅ Creating index 'arxiv-papers'...\n",
      "\n",
      "üìä Sample data to be indexed:\n",
      "|   id | submitter      | title                                                                                 | summary                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | authors                                              | comments                                | journal_ref              | doi                        | report_no        | categories     | license                                             | update_date         | published           |\n",
      "|-----:|:---------------|:--------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------|:----------------------------------------|:-------------------------|:---------------------------|:-----------------|:---------------|:----------------------------------------------------|:--------------------|:--------------------|\n",
      "|  704 | Pavel Nadolsky | Calculation of prompt diphoton production cross sections at Tevatron and LHC energies | A fully differential calculation in perturbative quantum chromodynamics is presented for the production of massive photon pairs at hadron colliders. All next-to-leading order perturbative contributions from quark-antiquark, gluon-(anti)quark, and gluon-gluon subprocesses are included, as well as all-orders resummation of initial-state gluon radiation valid at next-to-next-to-leading logarithmic accuracy. The region of phase space is specified in which the calculation is most reliable. Good agreement is demonstrated with data from the Fermilab Tevatron, and predictions are made for more detailed tests with CDF and DO data. Predictions are shown for distributions of diphoton pairs produced at the energy of the Large Hadron Collider (LHC). Distributions of the diphoton pairs from the decay of a Higgs boson are contrasted with those produced from QCD processes at the LHC, showing that enhanced sensitivity to the signal can be obtained with judicious selection of events. | C. Bal√°zs, E. L. Berger, P. M. Nadolsky, C. -P. Yuan | 37 pages, 15 figures; published version | Phys.Rev.D76:013009,2007 | 10.1103/PhysRevD.76.013009 | ANL-HEP-PR-07-12 | hep-ph         |                                                     | 2008-11-26T00:00:00 | 2007-04-02T19:18:42 |\n",
      "|  704 | Louis Theran   | Sparsity-certifying Graph Decompositions                                              | We describe a new algorithm, the $(k,\\ell)$-pebble game with colors, and use it obtain a characterization of the family of $(k,\\ell)$-sparse graphs and algorithmic solutions to a family of problems concerning tree decompositions of graphs. Special instances of sparse graphs appear in rigidity theory and have received increased attention in recent years. In particular, our colored pebbles generalize and strengthen the previous results of Lee and Streinu and give a new proof of the Tutte-Nash-Williams characterization of arboricity. We also present a new decomposition that certifies sparsity based on the $(k,\\ell)$-pebble game with colors. Our work also exposes connections between pebble game algorithms and previous sparse graph algorithms by Gabow, Gabow and Westermann and Hendrickson.                                                                                                                                                                                          | Ileana Streinu, Louis Theran                         | To appear in Graphs and Combinatorics   |                          |                            |                  | math.CO cs.CG  | http://arxiv.org/licenses/nonexclusive-distrib/1.0/ | 2008-12-13T00:00:00 | 2007-03-31T02:26:18 |\n",
      "|  704 | Hongjun Pan    | The evolution of the Earth-Moon system based on the dark matter field fluid model     | The evolution of Earth-Moon system is described by the dark matter field fluid model proposed in the Meeting of Division of Particle and Field 2004, American Physical Society. The current behavior of the Earth-Moon system agrees with this model very well and the general pattern of the evolution of the Moon-Earth system described by this model agrees with geological and fossil evidence. The closest distance of the Moon to Earth was about 259000 km at 4.5 billion years ago, which is far beyond the Roche's limit. The result suggests that the tidal friction may not be the primary cause for the evolution of the Earth-Moon system. The average dark matter field fluid constant derived from Earth-Moon system data is 4.39 x 10^(-22) s^(-1)m^(-1). This model predicts that the Mars's rotation is also slowing with the angular acceleration rate about -4.38 x 10^(-22) rad s^(-2).                                                                                                        | Hongjun Pan                                          | 23 pages, 3 figures                     |                          |                            |                  | physics.gen-ph |                                                     | 2008-01-13T00:00:00 | 2007-04-01T20:46:54 |\n",
      "\n",
      "üì§ Starting bulk indexing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [00:01<00:00, 5273.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Indexing complete. Total successful documents: 10000\n",
      "\n",
      "üöÄ Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "from kagglehub import dataset_download\n",
    "from tqdm import tqdm\n",
    "from requests.exceptions import ConnectionError\n",
    "\n",
    "# ---- Utility Functions ----\n",
    "def clean_text(text):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    return re.sub(r'\\s+', ' ', text.strip())\n",
    "\n",
    "def safe_date(date_str):\n",
    "    try:\n",
    "        return pd.to_datetime(date_str).isoformat()\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# ---- Preprocessing ----\n",
    "def preprocess_arxiv_json(json_file_path, max_records=None):\n",
    "    records = []\n",
    "    with open(json_file_path, 'r') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if max_records and i >= max_records:\n",
    "                break\n",
    "\n",
    "            try:\n",
    "                entry = json.loads(line)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "            # Parse authors\n",
    "            authors_list = entry.get('authors_parsed', [])\n",
    "            authors = [f\"{a[1]} {a[0]}\" for a in authors_list if isinstance(a, list)]\n",
    "            if not authors:\n",
    "                authors = [entry.get(\"authors\", \"\")]\n",
    "\n",
    "            # Parse publication date from versions[0]\n",
    "            published = \"\"\n",
    "            if isinstance(entry.get('versions'), list) and len(entry['versions']) > 0:\n",
    "                published = entry['versions'][0].get('created', '')\n",
    "\n",
    "            processed_entry = {\n",
    "                'id': entry.get('id'),\n",
    "                'submitter': entry.get('submitter', ''),\n",
    "                'title': clean_text(entry.get('title', '')),\n",
    "                'summary': clean_text(entry.get('abstract', '')),\n",
    "                'authors': ', '.join(authors),\n",
    "                'comments': entry.get('comments', ''),\n",
    "                'journal_ref': entry.get('journal-ref', ''),\n",
    "                'doi': entry.get('doi', ''),\n",
    "                'report_no': entry.get('report-no', ''),\n",
    "                'categories': entry.get('categories', ''),\n",
    "                'license': entry.get('license', '') or '',\n",
    "                'update_date': safe_date(entry.get('update_date', '')),\n",
    "                'published': safe_date(published)\n",
    "            }\n",
    "            records.append(processed_entry)\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "# ---- Elasticsearch Setup ----\n",
    "def create_index(es, index_name):\n",
    "    if es.indices.exists(index=index_name):\n",
    "        es.indices.delete(index=index_name)\n",
    "\n",
    "    index_config = {\n",
    "        \"settings\": {\n",
    "            \"number_of_shards\": 1,\n",
    "            \"number_of_replicas\": 0\n",
    "        },\n",
    "        \"mappings\": {\n",
    "            \"properties\": {\n",
    "                \"submitter\": {\"type\": \"text\"},\n",
    "                \"title\": {\"type\": \"text\"},\n",
    "                \"summary\": {\"type\": \"text\"},\n",
    "                \"authors\": {\"type\": \"text\"},\n",
    "                \"comments\": {\"type\": \"text\"},\n",
    "                \"journal_ref\": {\"type\": \"text\"},\n",
    "                \"doi\": {\"type\": \"keyword\"},\n",
    "                \"report_no\": {\"type\": \"keyword\"},\n",
    "                \"categories\": {\"type\": \"keyword\"},\n",
    "                \"license\": {\"type\": \"keyword\"},\n",
    "                \"update_date\": {\"type\": \"date\", \"format\": \"strict_date_optional_time||epoch_millis\"},\n",
    "                \"published\": {\"type\": \"date\", \"format\": \"strict_date_optional_time||epoch_millis\"}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    es.indices.create(index=index_name, body=index_config)\n",
    "\n",
    "# ---- Indexing with Sample Display ----\n",
    "def index_data(es, df, index_name):\n",
    "    print(\"\\nüìä Sample data to be indexed:\")\n",
    "    print(df.head(3).to_markdown(index=False))\n",
    "\n",
    "    print(\"\\nüì§ Starting bulk indexing...\")\n",
    "    successes = 0\n",
    "    failed_docs = []\n",
    "\n",
    "    actions = []\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Indexing\"):\n",
    "        doc = row.to_dict()\n",
    "        action = {\n",
    "            \"_index\": index_name,\n",
    "            \"_id\": doc[\"id\"],\n",
    "            \"_source\": doc\n",
    "        }\n",
    "        actions.append(action)\n",
    "\n",
    "        if len(actions) >= 500:\n",
    "            try:\n",
    "                resp = helpers.bulk(es, actions, raise_on_error=False)\n",
    "                successes += resp[0]\n",
    "            except Exception as e:\n",
    "                print(\"Error during bulk index:\", e)\n",
    "            actions = []\n",
    "\n",
    "    if actions:\n",
    "        try:\n",
    "            resp = helpers.bulk(es, actions, raise_on_error=False)\n",
    "            successes += resp[0]\n",
    "        except Exception as e:\n",
    "            print(\" Error during final batch indexing:\", e)\n",
    "\n",
    "    print(f\"\\n Indexing complete. Total successful documents: {successes}\")\n",
    "\n",
    "# ---- Connect to Elasticsearch ----\n",
    "def connect_to_elasticsearch_http(max_retries=10, wait_seconds=5):\n",
    "    es = None\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            es = Elasticsearch(\"http://localhost:9200\")\n",
    "            if es.ping():\n",
    "                print(\"Successfully connected to Elasticsearch via HTTP!\")\n",
    "                return es\n",
    "        except ConnectionError:\n",
    "            print(f\"Waiting for Elasticsearch (attempt {attempt + 1})...\")\n",
    "            time.sleep(wait_seconds)\n",
    "    raise Exception(\" Could not connect to Elasticsearch via HTTP after several attempts.\")\n",
    "\n",
    "# ---- Main Pipeline ----\n",
    "if __name__ == \"__main__\":\n",
    "    print(\" Downloading dataset...\")\n",
    "    dataset_path = dataset_download(\"Cornell-University/arxiv\")\n",
    "    print(\"Path to dataset files:\", dataset_path)\n",
    "\n",
    "    json_file_path = os.path.join(dataset_path, \"arxiv-metadata-oai-snapshot.json\")\n",
    "\n",
    "    print(\" Preprocessing data...\")\n",
    "    df = preprocess_arxiv_json(json_file_path, max_records=10000)\n",
    "\n",
    "    print(\" Connecting to Elasticsearch...\")\n",
    "    es = connect_to_elasticsearch_http()\n",
    "\n",
    "    index_name = \"arxiv-papers\"\n",
    "\n",
    "    print(f\" Creating index '{index_name}'...\")\n",
    "    create_index(es, index_name)\n",
    "\n",
    "    index_data(es, df, index_name)\n",
    "\n",
    "    print(\"\\n Done!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
